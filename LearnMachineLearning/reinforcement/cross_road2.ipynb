{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7887a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13945853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Environment setup\n",
    "GRID_SIZE = 5\n",
    "ACTIONS = ['left', 'right']\n",
    "action_map = {'left': -1, 'right': 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c84cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Q-table: rows = states, columns = actions\n",
    "Q = np.zeros((GRID_SIZE, len(ACTIONS)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8729516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1      # Learning rate\n",
    "gamma = 0.9      # Discount factor\n",
    "epsilon = 0.2    # Exploration rate\n",
    "episodes = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e171ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "for episode in range(episodes):\n",
    "    state = 0  # Start position\n",
    "\n",
    "    while state != GRID_SIZE - 1:\n",
    "        # Choose action (explore or exploit)\n",
    "        if random.random() < epsilon:\n",
    "            action_idx = random.choice([0, 1])\n",
    "        else:\n",
    "            action_idx = np.argmax(Q[state])\n",
    "\n",
    "        action = ACTIONS[action_idx]\n",
    "        next_state = state + action_map[action]\n",
    "\n",
    "        # Prevent out-of-bounds\n",
    "        next_state = max(0, min(GRID_SIZE - 1, next_state))\n",
    "\n",
    "        # Reward logic\n",
    "        reward = 10 if next_state == GRID_SIZE - 1 else -1\n",
    "\n",
    "        # Q-learning update\n",
    "        Q[state, action_idx] = Q[state, action_idx] + alpha * (\n",
    "            reward + gamma * np.max(Q[next_state]) - Q[state, action_idx]\n",
    "        )\n",
    "\n",
    "        state = next_state  # Move to next state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7e44301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-table:\n",
      " [[ 3.12198293  4.58      ]\n",
      " [ 3.12196776  6.2       ]\n",
      " [ 4.5799459   8.        ]\n",
      " [ 6.19997558 10.        ]\n",
      " [ 0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display learned Q-table\n",
    "print(\"Final Q-table:\\n\", Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e9b57f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¶ Agent path from start to goal:\n",
      "Path taken: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the agent\n",
    "print(\"\\nðŸš¶ Agent path from start to goal:\")\n",
    "state = 0\n",
    "path = [state]\n",
    "while state != GRID_SIZE - 1:\n",
    "    action_idx = np.argmax(Q[state])\n",
    "    action = ACTIONS[action_idx]\n",
    "    state += action_map[action]\n",
    "    state = max(0, min(GRID_SIZE - 1, state))\n",
    "    path.append(state)\n",
    "\n",
    "print(\"Path taken:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27896951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltgwgeorge-VLsDEmAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
