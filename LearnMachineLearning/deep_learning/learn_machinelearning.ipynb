{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b421aea9",
   "metadata": {},
   "source": [
    "Introduction to machine learning\n",
    "What is machine learninfg?\n",
    "- Machine learning is a subset of artificial intelligence that focuses on the development of algorithms and statistical models that enable computers to perform tasks without explicit instructions, relying instead on patterns and inference from data.\n",
    "- It involves training models on large datasets to make predictions or decisions based on new data.\n",
    "- Subset of AI that enables systems to learn from data and also experience without explicit programming them.\\\n",
    "\n",
    "Benefits of Machine Learning\n",
    "\n",
    "1. Improved Decision Making: Machine learning algorithms can analyze large volumes of data quickly and accurately, providing insights that help organizations make informed decisions.\n",
    "2. Automation of Repetitive Tasks: ML can automate routine tasks, freeing up human resources for more complex and creative work.\n",
    "3. Enhanced Customer Experiences: By analyzing customer data, ML can help businesses personalize their offerings and improve customer satisfaction.\n",
    "4. Predictive Analytics: Machine learning can identify trends and patterns in data, enabling organizations to anticipate future outcomes and make proactive decisions.\n",
    "5. Cost Savings: By optimizing processes and improving efficiency, machine learning can lead to significant cost reductions for businesses.\n",
    "6. Continuous Improvement: ML models can learn and adapt over time, improving their accuracy and effectiveness as they are exposed to more data.\n",
    "\n",
    "Types of Machine Learning\n",
    "1. Supervised Learning : In supervised learning, the model is trained on a labeled dataset, which means that each training example is paired with an output label. The model learns to map inputs to the correct output and can make predictions on new, unseen data.\n",
    "\n",
    "    Types\n",
    "    - Linear regression\n",
    "    - Logistuc regression\n",
    "    - Decision trees\n",
    "    - Support vector machines(SVM)\n",
    "    - K-nearest neighbours\n",
    "    - Random Forest\n",
    "    - Deep learning (CNN) : image classification\n",
    "\n",
    "2. Unsupervised Learning : Unsupervised learning involves training a model on data without labeled responses. The model tries to learn the underlying structure or distribution in the data to identify patterns or groupings.\n",
    "\n",
    "    Types\n",
    "    - K-means clustering\n",
    "    - Hierarchical clustering\n",
    "   \n",
    "3. Reinforcement Learning : Reinforcement learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize a reward signal. The agent learns from the consequences of its actions rather than from explicit examples.\n",
    "4. Semi-supervised Learning : Semi-supervised learning is a hybrid approach that combines labeled and unlabeled data for training. It is useful when acquiring a fully labeled dataset is expensive or time-consuming.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a00812b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (2.1.3)\n",
      "Requirement already satisfied: matplotlib in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: tensorflow in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: keras in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (3.10.0)\n",
      "Requirement already satisfied: scipy in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (1.15.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from matplotlib) (4.58.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (4.14.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (1.73.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: rich in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from keras) (14.0.0)\n",
      "Requirement already satisfied: namex in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from keras) (0.1.0)\n",
      "Requirement already satisfied: optree in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from keras) (0.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from rich->keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 12:03:03.566487: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-11 12:03:03.567146: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-11 12:03:03.570208: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-11 12:03:03.577958: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749632583.591176   94965 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749632583.594786   94965 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749632583.605231   94965 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749632583.605246   94965 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749632583.605248   94965 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749632583.605249   94965 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-11 12:03:03.608586: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy matplotlib tensorflow keras scipy\n",
    "\n",
    "#importing required libraries with explanations\n",
    "\n",
    "import os  # For interacting with the operating system (e.g., file paths)\n",
    "import numpy as np # type: ignore # For numerical operations and array handling\n",
    "import matplotlib.pyplot as plt # type: ignore # For data visualization and plotting\n",
    "import tensorflow as tf  # type: ignore # Main TensorFlow library for machine learning and deep learning\n",
    "from tensorflow import keras  # type: ignore # High-level API for building and training models\n",
    "\n",
    "# Model building and layers\n",
    "from tensorflow.keras.models import Sequential # type: ignore   # For creating sequential neural network models\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout  # Common neural network layers\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # For image data augmentation and preprocessing\n",
    "\n",
    "# Model training utilities\n",
    "from tensorflow.keras.optimizers import Adam  # Adam optimizer for training neural networks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint  # Callbacks for early stopping and saving best models\n",
    "\n",
    "# Pre-trained models\n",
    "from tensorflow.keras.applications import VGG16  # Pre-trained VGG16 model for transfer learning\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11da41da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define constants\n",
    "IMAGE_SIZE = (256, 256)  # Size to which images will be resized\n",
    "BATCH_SIZE = 32  # Number of samples per gradient update\n",
    "EPOCHS = 20  # Number of epochs to train the model\n",
    "NUM_CLASSES = 2  # Number of classes for crop disease detection (Healthy vs Diseased)\n",
    "ANIMAL_CLASSES = 3  # Number of classes for animal filtering (Dog, Cat, Human)\n",
    "# Paths\n",
    "DATASET_DIR = \"datasets\"  # Directory containing the dataset\n",
    "MODEL_PATH = \"agricure_model.h5\"  # Path to save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d00bb00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a CNN model for classification\n",
    "def create_model(input_shape, num_classes):\n",
    "    \"\"\"Create a CNN model for classification\"\"\"\n",
    "    model = Sequential([\n",
    "        #32 is for number of filters, (3, 3) is the kernel size\n",
    "        #Conv2D is a convolutional layer that applies a number of filters to the input image\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),#32 is for number of filters, (3, 3) is the kernel size\n",
    "        #MaxPooling2D reduces the spatial dimensions of the output volume like width and height by taking the maximum value in each patch of the feature map\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(256, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),# Flatten converts the 2D matrix into a 1D vector\n",
    "        Dropout(0.5),# Dropout is a regularization technique to prevent overfitting by randomly setting a fraction of input units to 0 at each update during training time\n",
    "        # Dropout layer helps to prevent overfitting by randomly setting a fraction of input units to 0 at each update during training time\n",
    "        # Dense layers are fully connected layers in a neural network\n",
    "        Dense(512, activation='relu'),\n",
    "        # The final layer is a Dense layer with softmax activation for multi-class classification\n",
    "        # The number of units in this layer should match the number of classes in your dataset\n",
    "        # Softmax activation function is used for multi-class classification problems\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2b8e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_crop_model():\n",
    "    \"\"\"Train the main crop disease detection model\"\"\"\n",
    "    # Create an ImageDataGenerator with augmentation and validation split\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split=0.2  # 20% for validation\n",
    "    )\n",
    "    \n",
    "    # Training generator (uses subset='training')\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        os.path.join(DATASET_DIR, \"crops\"),\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='training',  # Important!\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Validation generator (uses subset='validation')\n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        os.path.join(DATASET_DIR, \"crops\"),\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='validation',  # Important!\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Verify we found images\n",
    "    print(f\"Found {train_generator.samples} training images\")\n",
    "    print(f\"Found {validation_generator.samples} validation images\")\n",
    "    if train_generator.samples == 0 or validation_generator.samples == 0:\n",
    "        raise ValueError(\"No images found. Check your dataset structure and paths.\")\n",
    "\n",
    "    # Create model\n",
    "    model = create_model(IMAGE_SIZE + (3,), len(train_generator.class_indices))\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(CROP_MODEL_PATH, save_best_only=True),\n",
    "        EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    ]\n",
    "\n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    plot_training_history(history)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e1e3dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_animal_filter():\n",
    "    \"\"\"Train a secondary model to filter out animals/humans\"\"\"\n",
    "    animal_path = os.path.join(DATASET_DIR, \"animals\")\n",
    "    \n",
    "    # Debugging checks\n",
    "    if not os.path.exists(animal_path):\n",
    "        raise FileNotFoundError(f\"Animal directory not found at: {animal_path}\")\n",
    "    \n",
    "    print(\"\\nDataset contents:\")\n",
    "    for class_name in os.listdir(animal_path):\n",
    "        class_path = os.path.join(animal_path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            images = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            print(f\"{class_name}: {len(images)} images\")\n",
    "            if len(images) == 0:\n",
    "                print(f\"WARNING: No images found in {class_path}\")\n",
    "\n",
    "    animal_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    train_generator = animal_datagen.flow_from_directory(\n",
    "        animal_path,\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='training',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    validation_generator = animal_datagen.flow_from_directory(\n",
    "        animal_path,\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='validation',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining images: {train_generator.samples}\")\n",
    "    print(f\"Validation images: {validation_generator.samples}\")\n",
    "    \n",
    "    if train_generator.samples == 0:\n",
    "        raise ValueError(\"No training images found. Check that:\")\n",
    "        print(\"- Images exist in the subdirectories\")\n",
    "        print(\"- Images are in .jpg, .png, or .jpeg format\")\n",
    "        print(f\"- Directory structure: {animal_path}/classname/[images]\")\n",
    "\n",
    "    model = create_model(IMAGE_SIZE + (3,), len(train_generator.class_indices))\n",
    "    \n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=max(1, train_generator.samples // BATCH_SIZE),\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=max(1, validation_generator.samples // BATCH_SIZE)\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d9afabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation accuracy/loss\"\"\"\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs_range = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3e5ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_animal_filter():\n",
    "    \"\"\"Train a secondary model to filter out animals/humans\"\"\"\n",
    "    animal_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.2  # 20% for validation\n",
    "    )\n",
    "\n",
    "    # Training generator\n",
    "    train_generator = animal_datagen.flow_from_directory(\n",
    "        os.path.join(DATASET_DIR, \"animals\"),\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='training',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Validation generator\n",
    "    validation_generator = animal_datagen.flow_from_directory(\n",
    "        os.path.join(DATASET_DIR, \"animals\"),\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='validation',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Verify we found images\n",
    "    print(f\"Found {train_generator.samples} training images (animals)\")\n",
    "    print(f\"Found {validation_generator.samples} validation images (animals)\")\n",
    "    if train_generator.samples == 0 or validation_generator.samples == 0:\n",
    "        raise ValueError(\"No animal images found. Check your dataset structure and paths.\")\n",
    "\n",
    "    # Create model (number of classes is automatically determined from subdirectories)\n",
    "    model = create_model(IMAGE_SIZE + (3,), len(train_generator.class_indices))\n",
    "\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_generator.samples // BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60ab53fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models...\n",
      "\n",
      "Training crop disease model...\n",
      "Found 60 images belonging to 2 classes.\n",
      "Found 13 images belonging to 2 classes.\n",
      "Found 60 training images\n",
      "Found 13 validation images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "E0000 00:00:1749632602.738303   94965 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1749632602.738663   94965 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6071 - loss: 0.6856"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltgwgeorge/.local/share/virtualenvs/ltgwgeorge-VLsDEmAT/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.6071 - loss: 0.6856 - val_accuracy: 0.4615 - val_loss: 0.8081\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740ms/step - accuracy: 0.3750 - loss: 0.8445"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3750 - loss: 0.8445 - val_accuracy: 0.5385 - val_loss: 0.6375\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5714 - loss: 0.6736 - val_accuracy: 0.5385 - val_loss: 0.6586\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740ms/step - accuracy: 0.5000 - loss: 0.7397"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.7397 - val_accuracy: 0.5385 - val_loss: 0.6242\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5625 - loss: 0.6702"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5625 - loss: 0.6702 - val_accuracy: 0.6154 - val_loss: 0.5968\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668ms/step - accuracy: 0.5000 - loss: 0.6638"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.6638 - val_accuracy: 1.0000 - val_loss: 0.5711\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7857 - loss: 0.6272"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.7857 - loss: 0.6272 - val_accuracy: 0.9231 - val_loss: 0.5649\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745ms/step - accuracy: 0.7188 - loss: 0.6256"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.7188 - loss: 0.6256 - val_accuracy: 0.8462 - val_loss: 0.5445\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978ms/step - accuracy: 0.7857 - loss: 0.6183"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.7857 - loss: 0.6183 - val_accuracy: 0.8462 - val_loss: 0.5037\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745ms/step - accuracy: 0.6250 - loss: 0.6097"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.6250 - loss: 0.6097 - val_accuracy: 1.0000 - val_loss: 0.4557\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8214 - loss: 0.5184"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.8214 - loss: 0.5184 - val_accuracy: 1.0000 - val_loss: 0.3734\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722ms/step - accuracy: 0.8438 - loss: 0.5541"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.8438 - loss: 0.5541 - val_accuracy: 0.9231 - val_loss: 0.3714\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8125 - loss: 0.4994"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.8125 - loss: 0.4994 - val_accuracy: 0.9231 - val_loss: 0.3386\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657ms/step - accuracy: 0.7857 - loss: 0.5067"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.7857 - loss: 0.5067 - val_accuracy: 0.9231 - val_loss: 0.2920\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8571 - loss: 0.4966"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.8571 - loss: 0.4966 - val_accuracy: 1.0000 - val_loss: 0.2368\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712ms/step - accuracy: 0.8750 - loss: 0.3975"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.8750 - loss: 0.3975 - val_accuracy: 1.0000 - val_loss: 0.2098\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8571 - loss: 0.4373"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.8571 - loss: 0.4373 - val_accuracy: 1.0000 - val_loss: 0.1554\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744ms/step - accuracy: 0.8750 - loss: 0.3857"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.8750 - loss: 0.3857 - val_accuracy: 1.0000 - val_loss: 0.1405\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8571 - loss: 0.3176"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.8571 - loss: 0.3176 - val_accuracy: 1.0000 - val_loss: 0.1373\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750ms/step - accuracy: 0.8750 - loss: 0.3719"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.8750 - loss: 0.3719 - val_accuracy: 1.0000 - val_loss: 0.1287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop model saved to models/crop_model.h5\n",
      "\n",
      "Training animal filter model...\n",
      "Found 7 images belonging to 3 classes.\n",
      "Found 0 images belonging to 3 classes.\n",
      "Found 7 training images (animals)\n",
      "Found 0 validation images (animals)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No animal images found. Check your dataset structure and paths.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCrop model saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCROP_MODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining animal filter model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m animal_model = \u001b[43mtrain_animal_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m animal_model.save(ANIMAL_MODEL_PATH)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnimal model saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mANIMAL_MODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mtrain_animal_filter\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidation_generator.samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m validation images (animals)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train_generator.samples == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m validation_generator.samples == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo animal images found. Check your dataset structure and paths.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Create model (number of classes is automatically determined from subdirectories)\u001b[39;00m\n\u001b[32m     35\u001b[39m model = create_model(IMAGE_SIZE + (\u001b[32m3\u001b[39m,), \u001b[38;5;28mlen\u001b[39m(train_generator.class_indices))\n",
      "\u001b[31mValueError\u001b[39m: No animal images found. Check your dataset structure and paths."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define model paths (add these to your constants)\n",
    "    CROP_MODEL_PATH = \"models/crop_model.h5\"\n",
    "    ANIMAL_MODEL_PATH = \"models/animal_model.h5\"\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    # Train or load models\n",
    "    if not os.path.exists(CROP_MODEL_PATH) or not os.path.exists(ANIMAL_MODEL_PATH):\n",
    "        print(\"Training models...\")\n",
    "        \n",
    "        print(\"\\nTraining crop disease model...\")\n",
    "        crop_model = train_crop_model()\n",
    "        crop_model.save(CROP_MODEL_PATH)\n",
    "        print(f\"Crop model saved to {CROP_MODEL_PATH}\")\n",
    "        \n",
    "        print(\"\\nTraining animal filter model...\")\n",
    "        animal_model = train_animal_filter()\n",
    "        animal_model.save(ANIMAL_MODEL_PATH)\n",
    "        print(f\"Animal model saved to {ANIMAL_MODEL_PATH}\")\n",
    "    else:\n",
    "        print(\"Loading existing models...\")\n",
    "        try:\n",
    "            crop_model = tf.keras.models.load_model(CROP_MODEL_PATH)\n",
    "            animal_model = tf.keras.models.load_model(ANIMAL_MODEL_PATH)\n",
    "            print(\"Models loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading models: {e}\")\n",
    "            print(\"Retraining models...\")\n",
    "            crop_model = train_crop_model()\n",
    "            animal_model = train_animal_filter()\n",
    "\n",
    "    # Test prediction\n",
    "    test_image = \"test_image.jpg\"  # Replace with your test image\n",
    "    if os.path.exists(test_image):\n",
    "        try:\n",
    "            prediction = predict_image(crop_model, animal_model, test_image)\n",
    "            print(\"\\nPrediction Results:\")\n",
    "            print(f\"Type: {prediction['type']}\")\n",
    "            print(f\"Class: {prediction['class']}\")\n",
    "            print(f\"Confidence: {prediction['confidence']:.2%}\")\n",
    "            \n",
    "            # For debugging class mappings:\n",
    "            print(\"\\nClass Indices:\")\n",
    "            print(\"Crop classes:\", crop_model.class_indices if hasattr(crop_model, 'class_indices') else \"Not available\")\n",
    "            print(\"Animal classes:\", animal_model.class_indices if hasattr(animal_model, 'class_indices') else \"Not available\")\n",
    "        except Exception as e:\n",
    "            print(f\"Prediction failed: {e}\")\n",
    "    else:\n",
    "        print(f\"Test image {test_image} not found\")\n",
    "        print(\"Current directory contents:\", os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c5f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a384aa21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltgwgeorge-VLsDEmAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
