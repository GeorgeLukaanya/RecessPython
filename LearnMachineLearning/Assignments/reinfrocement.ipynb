{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f08d88c",
   "metadata": {},
   "source": [
    "Reinforcement learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward.\n",
    "\n",
    "Key concepts:\n",
    "- Agent: The learner or decision maker.\n",
    "- Environment: The world in which the agent operates.\n",
    "- Action: A decision made by the agent.\n",
    "- State: A situation in which the agent finds itself.\n",
    "- Reward: Feedback from the environment based on the action taken by the agent.\n",
    "- Policy: A strategy used by the agent to determine actions based on states.\n",
    "- Value Function: A function that estimates the expected cumulative reward from a state or state-action pair.\n",
    "- Exploration vs Exploitation: The dilemma of choosing between exploring new actions or exploiting known rewarding actions.\n",
    "\n",
    "\n",
    "Key characteristics of reinforcement learning\n",
    "1. No labels\n",
    "2. Involves exploration and exploitation\n",
    "3. Involves a reward signal\n",
    "\n",
    "Reinforcement learning algotithms\n",
    "1. Q-learning : A model-free reinforcement learning algorithm that learns the value of an action in a particular state.\n",
    "2. SARSA : An on-policy reinforcement learning algorithm that updates the action-value function based on the action taken.\n",
    "3. Deep Q-Networks (DQN) : Combines Q-learning with deep neural networks to handle high-dimensional state spaces.\n",
    "\n",
    "Q-learning algorithm\n",
    "Environment(position, goal, reward)\n",
    "actions = [left, right, up, down]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b50d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f0abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the environment\n",
    "position = 5\n",
    "actions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f5b00f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insitailize the Q-table\n",
    "Q = np.zeros((position, actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f43d4883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters\n",
    "episodes = 1000\n",
    "alpha = 0.8  # learning rate \n",
    "gamma = 0.9  # discount factor for future rewards\n",
    "epsilon = 0.3  # exploration rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43c9acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop\n",
    "for episode in range(episodes):\n",
    "    state = np.random.randint(0, position)  # Random initial state\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Choose action using epsilon-greedy policy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(0, actions)  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q[state])  # Exploit\n",
    "        \n",
    "        # Simulate environment response\n",
    "        next_state = (state + 1) % position if action == 0 else (state - 1) % position\n",
    "        reward = 1 if next_state == 0 else -1\n",
    "        \n",
    "        # Update Q-value\n",
    "        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if the episode is done\n",
    "        if state == 0:\n",
    "            done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e498843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table after training:\n",
      "         Action 0  Action 1\n",
      "State 0 -0.526315 -0.526315\n",
      "State 1 -1.473684  0.526316\n",
      "State 2 -1.473684 -0.526315\n",
      "State 3 -0.526315 -1.473684\n",
      "State 4  0.526316 -1.473684\n"
     ]
    }
   ],
   "source": [
    "# Convert Q-table to DataFrame for better visualization\n",
    "Q_df = pd.DataFrame(Q, columns=['Action 0', 'Action 1'], index=[f'State {i}' for i in range(position)])\n",
    "# Print the Q-table\n",
    "print(\"Q-table after training:\")\n",
    "print(Q_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1de7adb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1:\n",
      "State: 1, Action: 1\n",
      "Reached goal state!\n",
      "Episode 2:\n",
      "State: 1, Action: 1\n",
      "Reached goal state!\n",
      "Episode 3:\n",
      "State: 4, Action: 0\n",
      "Reached goal state!\n",
      "Episode 4:\n",
      "State: 2, Action: 1\n",
      "Moved to State: 1\n",
      "State: 1, Action: 1\n",
      "Reached goal state!\n",
      "Episode 5:\n",
      "State: 4, Action: 0\n",
      "Reached goal state!\n",
      "Episode 6:\n",
      "State: 2, Action: 1\n",
      "Moved to State: 1\n",
      "State: 1, Action: 1\n",
      "Reached goal state!\n",
      "Episode 7:\n",
      "State: 1, Action: 1\n",
      "Reached goal state!\n",
      "Episode 8:\n",
      "State: 0, Action: 1\n",
      "Moved to State: 4\n",
      "State: 4, Action: 0\n",
      "Reached goal state!\n",
      "Episode 9:\n",
      "State: 1, Action: 1\n",
      "Reached goal state!\n",
      "Episode 10:\n",
      "State: 0, Action: 1\n",
      "Moved to State: 4\n",
      "State: 4, Action: 0\n",
      "Reached goal state!\n"
     ]
    }
   ],
   "source": [
    "#Testing the learned policy\n",
    "test_episodes = 10\n",
    "for episode in range(test_episodes):\n",
    "    state = np.random.randint(0, position)  # Random initial state\n",
    "    done = False\n",
    "    print(f\"Episode {episode + 1}:\")\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(Q[state])  # Choose the best action\n",
    "        print(f\"State: {state}, Action: {action}\")\n",
    "        \n",
    "        # Simulate environment response\n",
    "        next_state = (state + 1) % position if action == 0 else (state - 1) % position\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if the episode is done\n",
    "        if state == 0:\n",
    "            done = True\n",
    "            print(\"Reached goal state!\")\n",
    "        else:\n",
    "            print(f\"Moved to State: {state}\")\n",
    "# End of the script\n",
    "# The script implements a simple reinforcement learning algorithm using Q-learning.\n",
    "# It initializes a Q-table, trains it over multiple episodes, and tests the learned policy.\n",
    "# The Q-table is printed at the end to show the learned values for each state-action pair.\n",
    "# The testing phase simulates episodes where the agent follows the learned policy to reach the goal state.\n",
    "# The script is a basic example of reinforcement learning and can be extended or modified for more complex environments.\n",
    "# The script implements a simple reinforcement learning algorithm using Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64e45032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table after training:\n",
      "         Action 0  Action 1\n",
      "State 0 -0.526315 -0.526315\n",
      "State 1 -1.473684  0.526316\n",
      "State 2 -1.473684 -0.526315\n",
      "State 3 -0.526315 -1.473684\n",
      "State 4  0.526316 -1.473684\n"
     ]
    }
   ],
   "source": [
    "# Convert Q-table to DataFrame for better visualization\n",
    "Q_df = pd.DataFrame(Q, columns=['Action 0', 'Action 1'], index=[f'State {i}' for i in range(position)])\n",
    "# Print the Q-table\n",
    "print(\"Q-table after training:\")\n",
    "print(Q_df)\n",
    "# Save the Q-table to a CSV file\n",
    "Q_df.to_csv('q_table.csv', index=True)\n",
    "# The script implements a simple reinforcement learning algorithm using Q-learning.         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d6c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltgwgeorge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
